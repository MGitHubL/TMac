import math
import numpy as np
from collections import defaultdict

# from torch_geometric.nn.conv.hgt_conv import group
from typing import Any, Dict, List, Optional, Union

import torch
from torch import Tensor
from torch.nn import Module, ModuleDict
from torch_geometric.typing import Adj, EdgeType, NodeType


def group(xs: List[Tensor], aggr: str) -> Optional[Tensor]:
    if len(xs) == 0:
        return None
    elif len(xs) == 1:
        return xs[0]
    elif aggr == "cat":
        return getattr(torch, aggr)(xs, dim=1)
    else:
        out = torch.stack(xs, dim=0)
        out = getattr(torch, aggr)(out, dim=0)
        out = out[0] if isinstance(out, tuple) else out
        return out


class HeteroConv(Module):
    r"""A generic wrapper for computing graph convolution on heterogeneous
    graphs.
    This layer will pass messages from source nodes to target nodes based on
    the bipartite GNN layer given for a specific edge type.
    If multiple relations point to the same destination, their results will be
    aggregated according to :attr:`aggr`.
    In comparison to :meth:`torch_geometric.nn.to_hetero`, this layer is
    especially useful if you want to apply different message passing modules
    for different edge types.

    .. code-block:: python

        hetero_conv = HeteroConv({
            ('paper', 'cites', 'paper'): GCNConv(-1, 64),
            ('author', 'writes', 'paper'): SAGEConv((-1, -1), 64),
            ('paper', 'written_by', 'author'): GATConv((-1, -1), 64),
        }, aggr='sum')

        out_dict = hetero_conv(x_dict, edge_index_dict)

        print(list(out_dict.keys()))
        >>> ['paper', 'author']

    Args:
        convs (Dict[Tuple[str, str, str], Module]): A dictionary
            holding a bipartite
            :class:`~torch_geometric.nn.conv.MessagePassing` layer for each
            individual edge type.
        aggr (string, optional): The aggregation scheme to use for grouping
            node embeddings generated by different relations.
            (:obj:`"sum"`, :obj:`"mean"`, :obj:`"min"`, :obj:`"max"`).
            (default: :obj:`"sum"`)
    """

    def __init__(self, convs: Dict[EdgeType, Module], aggr: str = "sum"):
        super().__init__()
        self.convs = ModuleDict({"__".join(k): v for k, v in convs.items()})
        self.aggr = aggr

    def reset_parameters(self):
        for conv in self.convs.values():
            conv.reset_parameters()

    def forward(
        self,
        x_dict: Dict[NodeType, Tensor],
        edge_index_dict: Dict[EdgeType, Adj],
        edge_weight_dict: Dict[EdgeType, Tensor],
        **kwargs_dict,
    ) -> Dict[NodeType, Tensor]:
        r"""
        Args:
            x_dict (Dict[str, Tensor]): A dictionary holding node feature
                information for each individual node type.
            edge_index_dict (Dict[Tuple[str, str, str], Tensor]): A dictionary
                holding graph connectivity information for each individual
                edge type.
            **kwargs_dict (optional): Additional forward arguments
                of individual :class:`torch_geometric.nn.conv.MessagePassing`
                layers.
                For example, if a specific GNN layer at edge type
                :obj:`edge_type` expects edge attributes :obj:`edge_attr` as a
                forward argument, then you can pass them to
                :meth:`~torch_geometric.nn.conv.HeteroConv.forward` via
                :obj:`edge_attr_dict = { edge_type: edge_attr }`.
        """
        out_dict = defaultdict(list)
        for edge_type, edge_index in edge_index_dict.items():
            src, rel, dst = edge_type

            str_edge_type = "__".join(edge_type)
            if str_edge_type not in self.convs:
                continue

            kwargs = {
                arg[:-5]: value[edge_type]  # `{*}_dict`
                for arg, value in kwargs_dict.items()
                if edge_type in value
            }

            conv = self.convs[str_edge_type]

            alpha = None

            if src == dst:
                out = conv(x=x_dict[src], edge_index=edge_index, edge_weight=edge_weight_dict[edge_type], **kwargs)
                # out = conv(x=x_dict[src], edge_index=edge_index, **kwargs)
            elif conv is not None:
                if "GAT" in conv._get_name():

                    out, alpha = conv(
                        x=(x_dict[src], x_dict[dst]),
                        edge_index=edge_index,
                        edge_weight=edge_weight_dict[edge_type],
                        return_attention_weights=True,
                        **kwargs,
                    )
                    '''
                    out, alpha = conv(
                        x=(x_dict[src], x_dict[dst]),
                        edge_index=edge_index,
                        return_attention_weights=True,
                        **kwargs,
                    )'''
                else:
                    out = conv(
                        x=(x_dict[src], x_dict[dst]), edge_index=edge_index, **kwargs
                    )

            out_dict[dst].append(out)

        for key, value in out_dict.items():
            out_dict[key] = group(value, self.aggr)

        return out_dict, alpha

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}(num_relations={len(self.convs)})"


def glorot(value: Any):
    if isinstance(value, Tensor):
        stdv = math.sqrt(6.0 / (value.size(-2) + value.size(-1)))
        value.data.uniform_(-stdv, stdv)
    else:
        for v in value.parameters() if hasattr(value, "parameters") else []:
            glorot(v)
        for v in value.buffers() if hasattr(value, "buffers") else []:
            glorot(v)


def zeros(value: Any):
    constant(value, 0.0)


def constant(value: Any, fill_value: float):
    if isinstance(value, Tensor):
        value.data.fill_(fill_value)
    else:
        for v in value.parameters() if hasattr(value, "parameters") else []:
            constant(v, fill_value)
        for v in value.buffers() if hasattr(value, "buffers") else []:
            constant(v, fill_value)


def pairwise_distances(x, y):
    '''
    Input: x is a Nxd matrix
           y is an optional Mxd matirx
    Output: dist is a NxM matrix where dist[i,j] is the square norm between x[i,:] and y[j,:]
            if y is not given then use 'y=x'.
    i.e. dist[i,j] = ||x[i,:]-y[j,:]||^2
    '''
    x_norm = (x ** 2).sum(1).view(-1, 1)
    if y is not None:
        y_t = torch.transpose(y, 0, 1)
        y_norm = (y ** 2).sum(1).view(1, -1)
    else:
        y_t = torch.transpose(x, 0, 1)
        y_norm = x_norm.view(1, -1)

    dist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)
    # Ensure diagonal is zero if x=y
    # if y is None:
    #     dist = dist - torch.diag(dist.diag)
    return torch.clamp(dist, 0.0, np.inf)

def num_of_graphs(ptr):
    """
    Count the number of graphs in the batch of data.
    """
    shifted_ptr = ptr.roll(1)
    shifted_ptr[0] = 0
    return ptr - shifted_ptr

